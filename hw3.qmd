---
title: "Homework 3"
author: "[Kate Miller]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
format: pdf
# format: pdf
---

---

::: {.callout-important style="font-size: 0.8em;"}

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::

For this assignment, we will be using the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset from the UCI Machine Learning Repository. The dataset consists of red and white _vinho verde_ wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests

We will be using the following libraries:

```{R, echo = TRUE}
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(car)
library(glmnet)
```

<br><br><br><br>
---
Unfortunately, my document would not render properly with the functions in the Appendix being at the bottom, so I had to move them up here. 
```{R, echo = TRUE}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

```{R, echo = TRUE}
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```
## Question 1
::: {.callout-tip}
## 50 points
Regression with categorical covariate and $t$-Test
:::

###### 1.1 (5 points)

Read the wine quality datasets from the specified URLs and store them in data frames `df1` and `df2`.

```{R, echo = TRUE}
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv" # Reads in the first csv

url2 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv" # Reads in the second csv


df1 <- read.csv(url1, sep = ";", header = TRUE)
df1
df2 <- read.csv(url2, sep = ";", header = TRUE) 
df2
```

---

###### 1.2 (5 points)

Perform the following tasks to prepare the data frame `df` for analysis:

1. Combine the two data frames into a single data frame `df`, adding a new column called `type` to indicate whether each row corresponds to white or red wine. 
1. Rename the columns of `df` to replace spaces with underscores
1. Remove the columns `fixed_acidity` and `free_sulfur_dioxide`
1. Convert the `type` column to a factor
1. Remove rows (if any) with missing values.


```{R, echo = TRUE}

df1$type <- "white" # Adds the 'type' column
df2$type <- "red"

df <- rbind(df1, df2) # Combines the data frames

colnames(df) <- gsub("\\.", "_", colnames(df)) # Renames columns to replace spaces with underscores

df <- df[, setdiff(colnames(df), c("fixed_acidity", "free_sulfur_dioxide"))] # Removes the requested columns

df$type <- as.factor(df$type) # Converts type column to a factor

df <- na.omit(df) # Removes rows with missing values (na)

dim(df) # Gives dimensions of data frame


```


Your output to `R dim(df)` should be
```
[1] 6497   11
```



---

###### 1.3 (20 points)

Recall from STAT 200, the method to compute the $t$ statistic for the the difference in means (with the equal variance assumption)

1. Using `df` compute the mean of `quality` for red and white wine separately, and then store the difference in means as a variable called `diff_mean`. 

2. Compute the pooled sample variance and store the value as a variable called `sp_squared`. 

3. Using `sp_squared` and `diff_mean`, compute the $t$ Statistic, and store its value in a variable called `t1`.


```{R, echo = TRUE}

mean_red <- mean(df$quality[df$type == "red"]) # Finds mean of red wine
mean_white <- mean(df$quality[df$type == "white"]) # Finds mean of white wine
diff_mean <- mean_red - mean_white # Finds the difference in the two means

var_red <- var(df$quality[df$type == "red"]) # Finds variance of red
var_white <- var(df$quality[df$type == "white"]) # Finds variance of white
num_red <- sum(df$type == "red") # Finds the total number of red
num_white <- sum(df$type == "white") # Finds the total number of white
sp_squared <- ((num_red - 1) * var_red + (num_white - 1) * var_white) / (num_red + num_white - 2) # Computes the pooled sample variance

t1 <- -(diff_mean / sqrt(sp_squared * (1/num_red + 1/num_white))) # Computes the t-statistic based on sp_squared. 
# I added a negative sign here to make the t-statistic positive.
t1

```


---

###### 1.4 (10 points)

Equivalently, R has a function called `t.test()` which enables you to perform a two-sample $t$-Test without having to compute the pooled variance and difference in means. 

Perform a two-sample t-test to compare the quality of white and red wines using the `t.test()` function with the setting `var.equal=TRUE`. Store the t-statistic in `t2`.

```{R, echo = TRUE}

t_test <- t.test(df[df$type == "white", "quality"], df[df$type == "red", "quality"], var.equal = TRUE) # Performs the t-test based on quality of both types of wine.
t2 <- t_test$statistic # Finds the test statistic.
t2
```

---

###### 1.5 (5 points)

Fit a linear regression model to predict `quality` from `type` using the `lm()` function, and extract the $t$-statistic for the `type` coefficient from the model summary. Store this $t$-statistic in `t3`.

```{R, echo = TRUE}

model <- lm(quality ~ type, data = df) # Fits the linear regression model to predict quality from type

summary_model <- summary(model)
t3 <- summary_model$coefficients[2, "t value"] # Extracts the t-statistic for the type coefficient.
t3
```


---

###### 1.6  (5 points)

Print a vector containing the values of `t1`, `t2`, and `t3`. What can you conclude from this? Why?

```{R}
c(t1, t2, t3) 
```
I can conclude that the different methods for calculating the t-statistic yield consistent/very similar results, since the t-statistics in all three cases were very close to one another.



<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 25 points
Collinearity
:::


---

###### 2.1 (5 points)

Fit a linear regression model with all predictors against the response variable `quality`. Use the `broom::tidy()` function to print a summary of the fitted model. What can we conclude from the model summary?


```{R, echo = TRUE}

library(broom)

full_model <- lm(quality ~ ., data = df) # Fits linear regression model with everything against quality

model_summary <- tidy(full_model) # Uses broom::tidy() to give a summary
print(model_summary)

```
From this model summary, we can conclude that all of the predictors show statistical significance when compared to the response variable "quality," since all predictors have p-values less than 5%. In this way, quality is meaningful in the model. For negative values, one variable may decrease as the other increases, and for positive values, one variable may increase as the other increases.

---

###### 2.2 (10 points)

Fit two **simple** linear regression models using `lm()`: one with only `citric_acid` as the predictor, and another with only `total_sulfur_dioxide` as the predictor. In both models, use `quality` as the response variable. How does your model summary compare to the summary from the previous question?


```{R, echo = TRUE}
model_citric_acid <- lm(quality ~ citric_acid, data = df) # Citric acid as predictor in model
summary_citric_acid <- tidy(model_citric_acid)
print(summary_citric_acid)

model_total_sulfur_dioxide <- lm(quality ~ total_sulfur_dioxide, data = df) # Total sulfur dioxide as predictor in model
summary_total_sulfur_dioxide <- tidy(model_total_sulfur_dioxide)
print(summary_total_sulfur_dioxide)


```
Both of these model summaries are very similar to the model summary in the previous question because we still see very small p-values, showing that statistical significance. The statistic for both model summaries does vary widely from zero in terms of standard deviation.  


---

###### 2.3 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{R, echo = TRUE}
library(corrplot)
correlation_matrix <- cor(df[sapply(df, is.numeric)]) # Obtains the numeric values to put into corrplot().

corrplot(correlation_matrix, method = "color") # Uses corrplot to visualize the correlation matrix.

```



---

###### 2.4 (5 points)

Compute the variance inflation factor (VIF) for each predictor in the full model using `vif()` function. What can we conclude from this?


```{R, echo = TRUE}

library(car)

model <- lm(quality ~ ., data = df) # Fits the linear regression model.

# Compute VIF
vif_values <- vif(model) # Computes variance inflation factor.

print(vif_values)

```
When the VIF is less than 5, there is a low level of multicollinearity. For instance, in volatile_acidity, total_silfur_dioxide, citric_acid, residual_sugar, pH, chlorides, and sulphates, there will be a low level of multicollinearity. For density and type, there is a moderate level of multicollinearity. Overall, the data suggests that there are no severe multicollinearity issues within the predictors.


<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 40 points

Variable selection
:::


---

###### 3.1 (5 points)

Run a backward stepwise regression using a `full_model` object as the starting model. Store the final formula in an object called `backward_formula` using the built-in `formula()` function in R

```{R, echo = TRUE}

full_model <- lm(quality ~ ., data = df) # Fits the full model

backward_model <- step(full_model, direction = "backward") # Uses the step function to do stepwise regression

backward_formula <- formula(backward_model) # Extracts the formula for the model

backward_formula


```

---

###### 3.2 (5 points)

Run a forward stepwise regression using a `null_model` object as the starting model. Store the final formula in an object called `forward_formula` using the built-in `formula()` function in R

```{R, echo = TRUE}

null_model <- lm(quality ~ ., data = df) # Fits the null model (I think the instructions meant to say full)

forward_model <- step(null_model, direction = "forward") # Makes forward model using stepwise regression

forward_formula <- formula(forward_model) # EXtrfacts formula from the model.

forward_formula
# I am not sure why it isn't as similar of an output to the previous question.

```



---

###### 3.3  (10 points)

1. Create a `y` vector that contains the response variable (`quality`) from the `df` dataframe. 

2. Create a design matrix `X` for the `full_model` object using the `make_model_matrix()` function provided in the Appendix. 

3. Then, use the `cv.glmnet()` function to perform LASSO and Ridge regression with `X` and `y`.

```{R, echo = TRUE}

y <- df$quality # Creates a y vector

X <- make_model_matrix(full_model) # Creates matrix X from the full model

library(glmnet)

lasso_model <- cv.glmnet(X, y, alpha = 1) # Performs lasso regression

ridge_model <- cv.glmnet(X, y, alpha = 0) # Performs ridge regression

lasso_model
ridge_model


```

Create side-by-side plots of the ridge and LASSO regression results. Interpret your main findings. 

```{R, echo = TRUE}
par(mfrow=c(1, 2)) # Creates side by side plots

plot(ridge_model, main = "Ridge Regression")

plot(lasso_model, main = "LASSO Regression")
```
The findings from these two plots are that they are very different. For instane, for the ridge regression, there are more values with low mean square error, but also substantial values with high mean square error. As for the lasso regression, there are many values with low mean square error and very few values with a high square error. As seen by the plots, they are very different shapes and widths. The ridge regression plot levels out at the end, whereas the lasso regression plot appears to continue to grow exponentially.

---

###### 3.4  (5 points)

Print the coefficient values for LASSO regression at the `lambda.1se` value? What are the variables selected by LASSO? 

Store the variable names with non-zero coefficients in `lasso_vars`, and create a formula object called `lasso_formula` using the `make_formula()` function provided in the Appendix. 

```{R, echo = TRUE}

lasso_coef <- coef(lasso_model, s = "lambda.1se") # Print coefficient values at lambda.1se
lasso_coef

lasso_vars <- rownames(lasso_coef) # Extracts variables with non-zero coefficients
lasso_vars

lasso_formula <- make_formula(lasso_vars)
lasso_formula

```
The variables selected by LASSO are volatile_acidity, citric_acid, residual_sugar, chlorides, total_sulfur_dioxide, density, pH, sulphates, alcohol, and type, but those with actual values include volatile_acidity, residual_sugar, sulphates, and alcohol.

---

###### 3.5  (5 points)

Print the coefficient values for ridge regression at the `lambda.1se` value? What are the variables selected here? 

Store the variable names with non-zero coefficients in `ridge_vars`, and create a formula object called `ridge_formula` using the `make_formula()` function provided in the Appendix. 

```{R, echo = TRUE}

ridge_coef <- coef(ridge_model, s = "lambda.1se") # Prints coefficient values at lambda.1se
ridge_coef

ridge_vars <- rownames(ridge_coef) # Extract variables with non-zero coefficients
ridge_vars

ridge_formula <- make_formula(ridge_vars)
ridge_formula


```
The variables selected here are volatile_acidity, residual_sugar, chlorides, total_sulfur_dioxide, density, pH, sulphates, alcohol, and type.

---

###### 3.6  (10 points)

What is the difference between stepwise selection, LASSO and ridge based on you analyses above?


Stepwise selection explicitly adds or removes variables based on certain criteria.
LASSO performs automatic variable selection by driving some coefficients to exactly zero.
Ridge regression makes the different values smaller, but it keeps all the variables. Stepwise selection sometimes involves choosing a significance level, whereas LASSO and ridge both use lambda. It is important to point out that stepwise selection does not use lambda as a tuning parameter.




<br><br><br><br>
<br><br><br><br>
---

## Question 4
::: {.callout-tip}
## 70 points

Variable selection
:::

---

###### 4.1  (5 points)

Excluding `quality` from `df` we have $10$ possible predictors as the covariates. How many different models can we create using any subset of these $10$ coavriates as possible predictors? Justify your answer. 

With 10 possible predictors, the formula is 2^n power, which equates to 2^10 power, which is 1024.

Therefore, we can create 1024 different models using any subset of the 10 covariates as possible predictors.

---


###### 4.2  (20 points)

Store the names of the predictor variables (all columns except `quality`) in an object called `x_vars`.

```{R, echo = TRUE}
x_vars <- colnames(df %>% select(-quality))
```

Use: 

* the `combn()` function (built-in R function) and 
* the `make_formula()` (provided in the Appendix) 

to **generate all possible linear regression formulas** using the variables in `x_vars`. This is most optimally achieved using the `map()` function from the `purrr` package.

```{R, echo = TRUE}
formulas <- map(
  1:length(x_vars),
  \(x){
    vars <- combn(x_vars, x, simplify = FALSE) # Using variables in x_vars
    map(vars, make_formula) # Uses the map function
  }
) %>% unlist()
head(formulas)

```

If your code is right the following command should return something along the lines of:

```{R, echo = TRUE}
sample(formulas, 4) %>% as.character()
# Output:
# [1] "quality ~ volatile_acidity + residual_sugar + density + pH + alcohol"                                                 
# [2] "quality ~ citric_acid"                                                                                                
# [3] "quality ~ volatile_acidity + citric_acid + residual_sugar + total_sulfur_dioxide + density + pH + sulphates + alcohol"
# [4] "quality ~ citric_acid + chlorides + total_sulfur_dioxide + pH + alcohol + type"  
```
My code did print out at least 100 formulas (one of all possible formulas I assume), there were just too many for me to print out altogether, so I used the head() function. 

---

###### 4.3  (10 points)
Use `map()` and `lm()` to fit a linear regression model to each formula in `formulas`, using `df` as the data source. Use `broom::glance()` to extract the model summary statistics, and bind them together into a single tibble of summaries using the `bind_rows()` function from `dplyr`.

```{R, echo = TRUE}
models <- map(formulas, ~lm(.x, data = df))  # Fits the linear regression model
summaries <- map(models, broom::glance) # Utilizes broom::glance()

combined_summaries <- bind_rows(summaries) # Binds them together to form a single tibble
combined_summaries
```



---


###### 4.4  (5 points)

Extract the `adj.r.squared` values from `summaries` and use them to identify the formula with the _**highest**_ adjusted R-squared value.

```{R, echo = TRUE}
adj_r_squared <- summaries %>%
  map_dbl("adj.r.squared") # Extracts the adj.r.squared values from summaries

highest_r <- which.max(adj_r_squared) # Finds the highest adj.r.squared value.

```

Store resulting formula as a variable called `rsq_formula`.

```{R, echo = TRUE}
rsq_formula <- formulas[highest_r]
rsq_formula
```

---

###### 4.5  (5 points)

Extract the `AIC` values from `summaries` and use them to identify the formula with the **_lowest_** AIC value.


```{R, echo = TRUE}
aic_values <- summaries %>%
  map_dbl("AIC") # Extracts AIC values from summaries
lowest_aic <- which.min(aic_values) # Identifies lowest AIC value.
```

Store resulting formula as a variable called `aic_formula`.


```{R, echo = TRUE}
aic_formula <- formulas[lowest_aic]
aic_formula
```

---

###### 4.6  (15 points)

Combine all formulas shortlisted into a single vector called `final_formulas`.

```{R, echo = TRUE}
null_formula <- formula(null_model)
full_formula <- formula(full_model)

final_formulas <- c(
  null_formula,
  full_formula,
  backward_formula,
  forward_formula,
  lasso_formula, 
  ridge_formula,
  rsq_formula,
  aic_formula
)
final_formulas
```

* Are `aic_formula` and `rsq_formula` the same? How do they differ from the formulas shortlisted in question 3?

The formulas obtained from AIC and adjusted R-squared focus on model fit, which makes them similar to each other. However, they are not going to be exactly the same. As for the formulas shortlisted in question 3, for instance, LASSO and Ridge formulas involve variable selection through regularization, which differs from how the aic_formula and rsq_formula were made and how they operate.

* Which of these is more reliable? Why? 

Each of them is reliable in its own way for different purposes. However, since LASSO and ridge tend to shrink their coefficients to approach zero, they may be more difficult to interpret than the AIC and R-squared formulas. Though, LASSO and ridge both also use lambda, which is useful for when considering parameter tuning. In conclusion, I would say that AIC and R-squared formulas would be more reliable since they normally result in models with fewer predictors.

* If we had a dataset with $10,000$ columns, which of these methods would you consider for your analyses? Why?

I would use the LASSO and ridge methods since they are more reliable for handling large amounts of data and perform variable selection, which is something that AIC and rsq formulas do not have.

---

###### 4.7  (10 points)


Use `map()` and `glance()` to extract the `sigma, adj.r.squared, AIC, df`, and `p.value` statistics for each model obtained from `final_formulas`. Bind them together into a single data frame `summary_table`. Summarize your main findings.

```{R, echo = TRUE}
#summary_table <- map(
  #final_formulas,
  #\(x) {
    #model <- lm(x, data = x_vars)
    #glance_output <- glance(model)
    #glance_output$model <- as.character(x)
    #glance_output[, c("model", "sigma", "adj.r.squared", "AIC", "df", "p.value")]
  #}
#) %>% bind_rows()

#summary_table %>% knitr::kable()




```

I unfortunately had to comment out my code to have my document render. I attempted to solve this question but was unable to with the given outline of how to do it, therefore making me unable to summarize my main findings.



:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---


# Appendix


#### Convenience function for creating a formula object

The following function which takes as input a vector of column names `x` and outputs a `formula` object with `quality` as the response variable and the columns of `x` as the covariates. 

```{R, echo = TRUE}
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

#### Convenience function for `glmnet`

The `make_model_matrix` function below takes a `formula` as input and outputs a **rescaled** model matrix `X` in a format amenable for `glmnet()`

```{R, echo = TRUE}
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```




::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::